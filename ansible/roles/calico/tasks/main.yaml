---
  - name: create /etc/calico directory
    file:
      path: "{{ calico_dir }}"
      state: directory

  - name: copy calicoctl.cfg to remote
    template:
      src: calicoctl.cfg.j2
      dest: "{{ calicoctl_conf_path }}"
      owner: "{{ kubernetes_owner }}"
      group: "{{ kubernetes_group }}"
      mode: "{{ network_environment_mode }}"

  - name: copy calico.yaml to remote
    template:
      src: calico.yaml
      dest: /etc/calico/calico.yaml
      owner: "{{ kubernetes_owner }}"
      group: "{{ kubernetes_group }}"
      mode: "{{ kubernetes_service_mode }}"

    # label nodes that will have have calico isntalled
  - name: label nodes for calico
    command: kubectl label nodes {% for host in play_hosts %}{{ host }} {% endfor %} kismatic/calico=true --overwrite --kubeconfig {{ kubernetes_kubeconfig_path }}
    run_once: true

  - name: start calico containers
    command: kubectl apply -f /etc/calico/calico.yaml --kubeconfig {{ kubernetes_kubeconfig_path }}
    run_once: true

  - name: get desired number of calico pods
    command: kubectl get ds calico-node -o=jsonpath='{.status.desiredNumberScheduled}' --namespace=kube-system --kubeconfig {{ kubernetes_kubeconfig_path }}
    register: desiredPods
    until: desiredPods|success
    retries: 20
    delay: 6
    run_once: true
  - name: wait until all calico pods are ready
    command: kubectl get ds calico-node -o=jsonpath='{.status.numberReady}' --namespace=kube-system --kubeconfig {{ kubernetes_kubeconfig_path }}
    register: readyPods
    until: desiredPods.stdout|int == readyPods.stdout|int
    retries: 20
    delay: 6
    failed_when: false # We don't want this task to actually fail (We catch the failure with a custom msg in the next task)
    run_once: true
  - name: fail if any calico pods are not ready
    fail:
      msg: "Timed out waiting for all calico pods to be ready."
    run_once: true
    when: desiredPods.stdout|int != readyPods.stdout|int
